{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yPHJvFfEbYch"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #used for data manipulation and analysis\n",
        "from sklearn.model_selection import train_test_split #split dataset for training and testing\n",
        "import math #python library for all mathematical functions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random #generate random numbers or selections\n",
        "from sklearn.manifold import TSNE #TSNE class tool to visualize high-dimensional data\n",
        "#t-sne is a dimensionality reduction technique used in data visualization and analysis\n",
        "import seaborn as sns #for creating statistical graphics\n",
        "import plotly.express as px\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm #used to create progress bars\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from  torch.utils.data import Dataset, DataLoader #for handling and loading data when working with PyTorch\n",
        "import cv2 #pip install opencv-python #image processing library\n",
        "import urllib #provides functions for working with URLs and web requests\n",
        "from colabcode import ColabCode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlkZVrTZySGE"
      },
      "source": [
        "# Variational Autoencode Implementation (VAE)\n",
        "\n",
        "- A type of neural network architecture used for generative modeling and dimensionality reduction\n",
        "- A type of artificial neural network and data compression\n",
        "\n",
        "\n",
        "1. **Autoencoder**: Think of an autoencoder as a system that tries to compress information. It has two main parts:\n",
        "   - **Encoder**: This part takes input data (like images, text, or any kind of information) and converts it into a compressed representation, often called a \"latent space\" or \"encoding.\" This encoding is a condensed version of the input data.\n",
        "   - **Decoder**: The decoder takes this compressed representation and tries to recreate the original input data from it. It's like a reverse process of the encoder.\n",
        "\n",
        "2. **Variational**: The \"variational\" part of VAE means that it introduces a level of randomness into the encoding process. Instead of producing a single fixed encoding for a given input, VAE generates a probability distribution of possible encodings. This adds a level of uncertainty to the process.\n",
        "\n",
        "\n",
        "- **Encoding with Variability**: When you feed data into the VAE, it doesn't just produce one fixed encoding. Instead, it produces a range of possible encodings, represented as a probability distribution. This variability is useful because it allows the VAE to capture the inherent uncertainty in real-world data.\n",
        "\n",
        "- **Sampling**: From this probability distribution, a point is randomly sampled. This sampled point becomes the encoding for the input data. This random sampling is a key feature of VAE and is what makes it \"variational.\"\n",
        "\n",
        "- **Decoding**: The sampled encoding is then passed through the decoder to generate a reconstructed version of the original input data.\n",
        "\n",
        "- **Training**: During the training process, VAE tries to minimize the difference between the original data and the reconstructed data. It also tries to make the distribution of encodings as close to a standard Gaussian distribution as possible. This encourages the model to learn a meaningful and structured latent space representation of the data.\n",
        "\n",
        "So, in essence, a Variational Autoencoder is a neural network that can both compress data into a meaningful latent space and generate data from points in that latent space while introducing a level of randomness. This randomness makes it a powerful tool for various applications, including image generation, data denoising, and more, as it can capture the uncertainty and diversity present in real-world data.\n",
        "\n",
        "How it works:\n",
        "  encoder -> latent space -> decoder -> variational part\n",
        "\n",
        "**Encoder**\n",
        "  - takes the picture and tries to learn a compressed representation of it\n",
        "  - looks for essential features (e.g eyes, ears, snout, etc.)\n",
        "  - hidden code\n",
        "\n",
        "**Latent Space**\n",
        "  - special space where each point represents a unique feature decoder\n",
        "  - a place for hidden codes from the encoder\n",
        "\n",
        "**Decoder**\n",
        " - the decoder has learned how to tranform these latent codes into recognizable images\n",
        " - variational part\n",
        "    - in this part, it makes VAE special; it can generate slight difference from the original\n",
        "\n",
        "Directory Structure\n",
        "- input\n",
        "    - data\n",
        "- outputs\n",
        "- src\n",
        "    - model.py\n",
        "    - train.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGOBb3DWsBCN"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "  #constructor to create a VAE object, allowing parameters num_feature and num_dim\n",
        "  def __init__(self, num_features = 8, num_dim = 784): #28x28 pixels\n",
        "    super(VariationalAutoencoder, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.num_dim = num_dim\n",
        "\n",
        "    #creates a neural network layer that will be used for encoding input data\n",
        "    self.encoder_layer_1 = nn.Linear(in_features = self.num_dim, out_features = 512)\n",
        "\n",
        "    #creates another encoding layer that takes 512 features from the previous layer\n",
        "    #and produces num_features * 2\n",
        "    self.encoder_layer2 = nn.Linear(in_features = 512, out_features = (self.num_features*2))\n",
        "\n",
        "    #code responsible for reconstruction of data from the learned latent space\n",
        "    self.decoder_layer_1 = nn.Linear(in_features = self.num_features, out_features = 512)\n",
        "    self.decoder_layer_2 = nn.Linear(in_features = 512, out_features = (self.num_dim))\n",
        "\n",
        "\n",
        "  def reparameterize(self, mu, log_var):\n",
        "    '''\n",
        "    :param mu: mean from the encoder's latent space\n",
        "        - is like the desired average or center point inside the box\n",
        "    :param log_var: log variance from the encoder's latent space\n",
        "        - is like how spread out you want your random stuff to be inside the box(higher values mean more spread)\n",
        "    '''\n",
        "    std = torch.exp(0.5 * log_var) #standard deviation\n",
        "    eps = torch.randn_like(std) #randn_like as we need the same size\n",
        "    sample = mu + (eps * std) #sampling as if coming from the input space\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def encode(self, x):\n",
        "    #encoding\n",
        "    x = F.relu(self.encoder_layer_1(x))\n",
        "    #relu (rectified linear unit) - a mathematical function commonly used in AI neural network\n",
        "    #ReLu(x) = max(0,x)\n",
        "    x = self.encoder_layer_2(x).view(-1, 2, self.num_features)\n",
        "\n",
        "    #get mu and log_var\n",
        "    mu = x[:, 0, :]\n",
        "    log_var = x[:, 1, :]\n",
        "\n",
        "    z = self.reparameterize(mu, log_var)\n",
        "\n",
        "    return z, mu, log_var\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxL0i9RRyzCb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
